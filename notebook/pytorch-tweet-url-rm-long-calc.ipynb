{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import re\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tokenizers\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, max_len=96):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = 'selected_text' in df\n",
    "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            vocab_file='../input/roberta-base/vocab.json', \n",
    "            merges_file='../input/roberta-base/merges.txt', \n",
    "            lowercase=True,\n",
    "            add_prefix_space=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {}\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        ids, masks, tweet, offsets = self.get_input_data(row)\n",
    "        data['ids'] = ids\n",
    "        data['masks'] = masks\n",
    "        data['tweet'] = tweet\n",
    "        data['offsets'] = offsets\n",
    "        \n",
    "        if self.labeled:\n",
    "            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
    "            data['start_idx'] = start_idx\n",
    "            data['end_idx'] = end_idx\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_input_data(self, row):\n",
    "        tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "        encoding = self.tokenizer.encode(tweet)\n",
    "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n",
    "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
    "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "                \n",
    "        pad_len = self.max_len - len(ids)\n",
    "        if pad_len > 0:\n",
    "            ids += [1] * pad_len\n",
    "            offsets += [(0, 0)] * pad_len\n",
    "        \n",
    "        ids = torch.tensor(ids)\n",
    "        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "        offsets = torch.tensor(offsets)\n",
    "        \n",
    "        return ids, masks, tweet, offsets\n",
    "        \n",
    "    def get_target_idx(self, row, tweet, offsets):\n",
    "        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "        len_st = len(selected_text) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "\n",
    "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "            if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                break\n",
    "\n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for ct in range(idx0, idx1 + 1):\n",
    "                char_targets[ct] = 1\n",
    "\n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(offsets):\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "\n",
    "        start_idx = target_idx[0]\n",
    "        end_idx = target_idx[-1]\n",
    "        \n",
    "        return start_idx, end_idx\n",
    "        \n",
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TweetModel, self).__init__()\n",
    "        \n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            '../input/roberta-base/config.json', output_hidden_states=True)    \n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            '../input/roberta-base/pytorch_model.bin', config=config)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(config.hidden_size, 2)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
    "         \n",
    "        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
    "        x = torch.mean(x, 0)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        start_logits, end_logits = x.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "                \n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    end_loss = ce_loss(end_logits, end_positions)    \n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_text(text, start_idx, end_idx, offsets):\n",
    "    selected_text = \"\"\n",
    "    for ix in range(start_idx, end_idx + 1):\n",
    "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"\n",
    "    return selected_text\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
    "    start_pred = np.argmax(start_logits)\n",
    "    end_pred = np.argmax(end_logits)\n",
    "    if start_pred > end_pred:\n",
    "        pred = text\n",
    "    else:\n",
    "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
    "        \n",
    "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
    "    \n",
    "    return jaccard(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
    "    model.cuda()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_jaccard = 0.0\n",
    "            \n",
    "            for data in (dataloaders_dict[phase]):\n",
    "                ids = data['ids'].cuda()\n",
    "                masks = data['masks'].cuda()\n",
    "                tweet = data['tweet']\n",
    "                offsets = data['offsets'].numpy()\n",
    "                start_idx = data['start_idx'].cuda()\n",
    "                end_idx = data['end_idx'].cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    start_logits, end_logits = model(ids, masks)\n",
    "\n",
    "                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() * len(ids)\n",
    "                    \n",
    "                    start_idx = start_idx.cpu().detach().numpy()\n",
    "                    end_idx = end_idx.cpu().detach().numpy()\n",
    "                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
    "                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
    "                    \n",
    "                    for i in range(len(ids)):                        \n",
    "                        jaccard_score = compute_jaccard_score(\n",
    "                            tweet[i],\n",
    "                            start_idx[i],\n",
    "                            end_idx[i],\n",
    "                            start_logits[i], \n",
    "                            end_logits[i], \n",
    "                            offsets[i])\n",
    "                        epoch_jaccard += jaccard_score\n",
    "                    \n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
    "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
    "    \n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%%time\\n\\ntrain_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\\ntrain_df['text'] = train_df['text'].astype(str)\\ntrain_df['selected_text'] = train_df['selected_text'].astype(str)\\n\\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \\n    print(f'Fold: {fold}')\\n\\n    model = TweetModel()\\n    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\\n    criterion = loss_fn    \\n    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\\n\\n    train_model(\\n        model, \\n        dataloaders_dict,\\n        criterion, \\n        optimizer, \\n        num_epochs,\\n        f'roberta_fold{fold}.pth')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "train_df['selected_text'] = train_df['selected_text'].astype(str)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
    "    print(f'Fold: {fold}')\n",
    "\n",
    "    model = TweetModel()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "    criterion = loss_fn    \n",
    "    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "\n",
    "    train_model(\n",
    "        model, \n",
    "        dataloaders_dict,\n",
    "        criterion, \n",
    "        optimizer, \n",
    "        num_epochs,\n",
    "        f'roberta_fold{fold}.pth')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 44.5 s, total: 2min 39s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df['text'] = test_df['text'].astype(str)\n",
    "test_loader = get_test_loader(test_df)\n",
    "predictions = []\n",
    "models = []\n",
    "for fold in range(skf.n_splits):\n",
    "    model = TweetModel()\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(f'../input/tweet-sentiment-roberta-pytorch/roberta_fold{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "for data in test_loader:\n",
    "    ids = data['ids'].cuda()\n",
    "    masks = data['masks'].cuda()\n",
    "    tweet = data['tweet']\n",
    "    offsets = data['offsets'].numpy()\n",
    "\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(ids, masks)\n",
    "            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "    start_logits = np.mean(start_logits, axis=0)\n",
    "    end_logits = np.mean(end_logits, axis=0)\n",
    "    for i in range(len(ids)):    \n",
    "        start_pred = np.argmax(start_logits[i])\n",
    "        end_pred = np.argmax(end_logits[i])\n",
    "        if start_pred > end_pred:\n",
    "            pred = tweet[i]\n",
    "        else:\n",
    "            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rm_url(text): \n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    return ''.join(text)\n",
    "def rm_underbar(text):\n",
    "    text = re.sub('^_.{2,}?\\s', '', text)\n",
    "    return ''.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = 'i like _73984 the curry rice'\n",
    "#rm_underbar(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>such a shame!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>i like it!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>726e501993</td>\n",
       "      <td>that`s great!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>261932614e</td>\n",
       "      <td>hates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>afa11da83f</td>\n",
       "      <td>blocked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e64208b4ef</td>\n",
       "      <td>and within a short time of the last clue all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37bcad24ca</td>\n",
       "      <td>What did you get?  My day is alright.. haven`...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24c92644a4</td>\n",
       "      <td>argh total bummer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>43b390b336</td>\n",
       "      <td>I checked.  We didn`t win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>69d6b5d93e</td>\n",
       "      <td>.. and you`re on twitter! Did the tavern bore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5c1e0b61a1</td>\n",
       "      <td>sad,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>504e45d9d9</td>\n",
       "      <td>i feel like my phones hole is not a virgin. t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ae93ad52a0</td>\n",
       "      <td>hate my new timetable, having such a bad week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9fce30159a</td>\n",
       "      <td>Miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d5195223</td>\n",
       "      <td>cramps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33f19050cf</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>f7718b3c23</td>\n",
       "      <td>I`m going into a spiritual stagnentation, its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9ef44428d0</td>\n",
       "      <td>stupid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>be634ebeb0</td>\n",
       "      <td>dead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3dcf4f7e13</td>\n",
       "      <td>bad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>f0ef04109b</td>\n",
       "      <td>about to go to sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8be365118e</td>\n",
       "      <td>lame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>c50bdd4567</td>\n",
       "      <td>thats so cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>334954f215</td>\n",
       "      <td>hey peoples, dont you just hate being grounded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>b783916431</td>\n",
       "      <td>Huh, another ScarePoint coding Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1fa8e6ad66</td>\n",
       "      <td>look who i found just for you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>be38b29042</td>\n",
       "      <td>no ac, the fan doesnt swing our way ... we ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>95701e8ed9</td>\n",
       "      <td>guess that depends on if you want to be on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4dd58ecd2c</td>\n",
       "      <td>resorted to eating Mickey Ds ALONE.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cf553cccd5</td>\n",
       "      <td>There is a faux gothy chick looking at me, sor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>55c69c2d52</td>\n",
       "      <td>did he ask for your Twitter ID? Your sun sign?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9b1a52cc02</td>\n",
       "      <td>where`d you go!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>cce5c4744a</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>c7f9d559e2</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3d2478db59</td>\n",
       "      <td>So I really need to put the laptop down &amp; star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6d846d7d50</td>\n",
       "      <td>i`m sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>599f51af6b</td>\n",
       "      <td>feels sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>87daa62920</td>\n",
       "      <td>u tweet too much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6506dc55ff</td>\n",
       "      <td>I always forget SOMETHING when I travel. I am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>746d87f93e</td>\n",
       "      <td>Should have left car and walked home! I might ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ef7ef8e474</td>\n",
       "      <td>i`m only updating this so that brett`s phone b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>d3d7b13278</td>\n",
       "      <td>happy,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ddc07db84e</td>\n",
       "      <td>hope ur havin fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>67afe60d31</td>\n",
       "      <td>i miss my old phone it worked so good until i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>17d18ba9ef</td>\n",
       "      <td>wonderful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ab57d75f20</td>\n",
       "      <td>im really sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>f0acd25d8e</td>\n",
       "      <td>Not happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textID                                      selected_text\n",
       "0   f87dea47db                          Last session of the day  \n",
       "1   96d74cb729                                           exciting\n",
       "2   eee518ae67                                      such a shame!\n",
       "3   01082688c6                                        happy bday!\n",
       "4   33987a8ee5                                        i like it!!\n",
       "5   726e501993                                     that`s great!!\n",
       "6   261932614e                                              hates\n",
       "7   afa11da83f                                            blocked\n",
       "8   e64208b4ef   and within a short time of the last clue all ...\n",
       "9   37bcad24ca   What did you get?  My day is alright.. haven`...\n",
       "10  24c92644a4                                  argh total bummer\n",
       "11  43b390b336                          I checked.  We didn`t win\n",
       "12  69d6b5d93e   .. and you`re on twitter! Did the tavern bore...\n",
       "13  5c1e0b61a1                                               sad,\n",
       "14  504e45d9d9   i feel like my phones hole is not a virgin. t...\n",
       "15  ae93ad52a0      hate my new timetable, having such a bad week\n",
       "16  9fce30159a                                           Miss you\n",
       "17  00d5195223                                             cramps\n",
       "18  33f19050cf                                               nice\n",
       "19  f7718b3c23  I`m going into a spiritual stagnentation, its ...\n",
       "20  9ef44428d0                                             stupid\n",
       "21  be634ebeb0                                               dead\n",
       "22  3dcf4f7e13                                               bad.\n",
       "23  f0ef04109b                               about to go to sleep\n",
       "24  8be365118e                                               lame\n",
       "25  c50bdd4567                                      thats so cool\n",
       "26  334954f215  hey peoples, dont you just hate being grounded...\n",
       "27  b783916431              Huh, another ScarePoint coding Sunday\n",
       "28  1fa8e6ad66                      look who i found just for you\n",
       "29  be38b29042   no ac, the fan doesnt swing our way ... we ar...\n",
       "30  95701e8ed9   guess that depends on if you want to be on th...\n",
       "31  4dd58ecd2c                resorted to eating Mickey Ds ALONE.\n",
       "32  cf553cccd5  There is a faux gothy chick looking at me, sor...\n",
       "33  55c69c2d52     did he ask for your Twitter ID? Your sun sign?\n",
       "34  9b1a52cc02                                    where`d you go!\n",
       "35  cce5c4744a                                               good\n",
       "36  c7f9d559e2                                              happy\n",
       "37  3d2478db59  So I really need to put the laptop down & star...\n",
       "38  6d846d7d50                                          i`m sorry\n",
       "39  599f51af6b                                        feels sorry\n",
       "40  87daa62920                                   u tweet too much\n",
       "41  6506dc55ff   I always forget SOMETHING when I travel. I am...\n",
       "42  746d87f93e  Should have left car and walked home! I might ...\n",
       "43  ef7ef8e474  i`m only updating this so that brett`s phone b...\n",
       "44  d3d7b13278                                             happy,\n",
       "45  ddc07db84e                                  hope ur havin fun\n",
       "46  67afe60d31  i miss my old phone it worked so good until i ...\n",
       "47  17d18ba9ef                                          wonderful\n",
       "48  ab57d75f20                                    im really sorry\n",
       "49  f0acd25d8e                                          Not happy"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "#sub_df['selected_text'] = sub_df['selected_text'].astype(str)\n",
    "sub_df['selected_text'] = predictions\n",
    "\n",
    "#post proceccing\n",
    "for k in range(sub_df.shape[0]):\n",
    "    if test_df.loc[k, 'sentiment'] == 'neutral':\n",
    "        sub_df.loc[k, 'selected_text'] = rm_url(test_df.loc[k, 'text'])\n",
    "    \n",
    "    elif test_df.loc[k, 'sentiment'] == 'positive' or 'negative':\n",
    "        if len(test_df.loc[k, 'text'].split()) <= 2:\n",
    "            sub_df.loc[k, 'selected_text'] = rm_url(test_df.loc[k, 'text'])\n",
    "    \n",
    "    \n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
    "\n",
    "\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "sub_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
