{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_tw_senti_preprocess",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qFR9QdN9jH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "36d746c5-7354-4b09-ba98-c88a560de982"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjLUFmQAhWl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(tensorflow)\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0cbDRL_hq7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0beb2db6-fea0-4144-ac95-56c47e819294"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA0moWFk-CP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e64f59c-6da3-4fe5-b49b-2c8fd673fd91"
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFMYgwDS-U1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!kaggle competitions list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpY3Tquf-1og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3ca9629d-97b1-41fc-a03f-5bfca4187688"
      },
      "source": [
        "!kaggle competitions download -c tweet-sentiment-extraction "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/307k [00:00<?, ?B/s]\n",
            "100% 307k/307k [00:00<00:00, 46.2MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/41.4k [00:00<?, ?B/s]\n",
            "100% 41.4k/41.4k [00:00<00:00, 43.1MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "  0% 0.00/1.23M [00:00<?, ?B/s]\n",
            "100% 1.23M/1.23M [00:00<00:00, 83.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbvG-SZTBdKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "bee51db8-b64b-4674-962d-acb01f48f83f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=b4cd874ee2e1747a247190a6512cd2f20afe3e9390989015065d73505ae5a7d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK-BnlsZ8Ecd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import seaborn as sns; sns.set(style='white')\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import math\n",
        "import re\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import concatenate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkM1YUDV8Ee0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f562c67c-c81a-4a62-e01a-08990e18e064"
      },
      "source": [
        "MAX_LEN = 96\n",
        "PATH = 'drive/My Drive/tf-roberta/'\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=PATH+'vocab-roberta-base.json', \n",
        "    merges_file=PATH+'merges-roberta-base.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "EPOCHS = 3 # originally 3\n",
        "BATCH_SIZE = 32 # originally 32\n",
        "PAD_ID = 1\n",
        "SEED = 88888\n",
        "LABEL_SMOOTHING = 0.1\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
        "train = pd.read_csv('drive/My Drive/tw_senti_com/train.csv').fillna('')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... sentiment\n",
              "0  cb774db0d1  ...   neutral\n",
              "1  549e992a42  ...  negative\n",
              "2  088c60f138  ...  negative\n",
              "3  9642c003ef  ...  negative\n",
              "4  358bd9e861  ...  negative\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXd5UjRXH4Up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#先頭なら問答無用でそのまま出力\n",
        "#先頭なら後ろにスペースがあってもそのまま出力。\n",
        "def preprocess_txt(text1, text2):\n",
        "  sp_loc = text1.find('  ')\n",
        "  comma_loc = text1.find('`')\n",
        "  sel_loc = text1.find(text2)\n",
        "  if sp_loc != -1 and sp_loc <= sel_loc + 1:\n",
        "    if len(text2.split()[0]) == 1:\n",
        "      return text2.strip()[1:].strip()\n",
        "  #elif comma_loc != -1 and comma_loc <= sel_loc + 1:\n",
        "    #if len(text2.strip().split()[0]) != 0:\n",
        "      #return text2.strip()[1:]\n",
        "    else:\n",
        "      return text2\n",
        "  else:\n",
        "    return text2\n",
        "#シャープ版も作ってね!\n",
        "#¿½も\n",
        "#gooはgoodに変換する。\n",
        "#単語の途中で始まるものは左の一語の一文字だけ取り除く\n",
        "#選択したテキストないの'  ' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToUX2JiNjdxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "584270fa-8712-4626-abd9-6e10d6652122"
      },
      "source": [
        "preprocess_txt('i like  curry rice very much', 'y much')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'much'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaoQRLvZiVPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1e507b4-fce3-4b54-c0ed-c5e84019d32d"
      },
      "source": [
        "import re \n",
        "def preprocess_all_tw(txt):\n",
        "  txt = txt.strip()\n",
        "  return re.sub(r'\\ {1,}', r' ', txt)\n",
        "\n",
        "text_sam = ' i dont like  curry   rice'\n",
        "preprocess_all_tw(text_sam)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i dont like curry rice'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjXIYC-j9mdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from nltk.corpus import words\n",
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYXMX_Of_Ct8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"train.csv.zip\")\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOQE9m0p8Ehc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv('test.csv').fillna('')\n",
        "\n",
        "ct = test.shape[0]\n",
        "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(test.shape[0]):\n",
        "        \n",
        "    # INPUT_IDS\n",
        "    all_tweet_test = test.loc[k,'text']\n",
        "    all_tweet_test = preprocess_all_tw(all_tweet_test)\n",
        "    text1 = \" \"+\" \".join(all_tweet_test.split())\n",
        "    enc = tokenizer.encode(text1)                \n",
        "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
        "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
        "    attention_mask_t[k,:len(enc.ids)+3] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCl5OmUc8EkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dropout_new = 0.15     # originally 0.1\n",
        "n_split = 5            # originally 5\n",
        "lr = 3e-5              # originally 3e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN7_ZvpD8Emj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def save_weights(model, dst_fn):\n",
        "    weights = model.get_weights()\n",
        "    with open(dst_fn, 'wb') as f:\n",
        "        pickle.dump(weights, f)\n",
        "\n",
        "\n",
        "def load_weights(model, weight_fn):\n",
        "    with open(weight_fn, 'rb') as f:\n",
        "        weights = pickle.load(f)\n",
        "    model.set_weights(weights)\n",
        "    return model\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    # adjust the targets for sequence bucketing\n",
        "    ll = tf.shape(y_pred)[1]\n",
        "    y_true = y_true[:, :ll]\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
        "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
        "\n",
        "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
        "    max_len = tf.reduce_max(lens)\n",
        "    ids_ = ids[:, :max_len]\n",
        "    att_ = att[:, :max_len]\n",
        "    tok_ = tok[:, :max_len]\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\n",
        "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(Dropout_new)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
        "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
        "    \n",
        "    # this is required as `model.predict` needs a fixed size!\n",
        "    #'右'だけpadで埋める\n",
        "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    \n",
        "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
        "    return model, padded_model    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0vpJeGOW4lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def save_weights(model, dst_fn):\n",
        "    weights = model.get_weights()\n",
        "    with open(dst_fn, 'wb') as f:\n",
        "        pickle.dump(weights, f)\n",
        "\n",
        "def load_weights(model, weight_fn):\n",
        "    with open(weight_fn, 'rb') as f:\n",
        "        weights = pickle.load(f)\n",
        "    model.set_weights(weights)\n",
        "    return model\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    # adjust the targets for sequence bucketing\n",
        "    ll = tf.shape(y_pred)[1]\n",
        "    y_true = y_true[:, :ll]\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
        "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "def build_model_lstm():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
        "\n",
        "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
        "    max_len = tf.reduce_max(lens)\n",
        "    ids_ = ids[:, :max_len]\n",
        "    att_ = att[:, :max_len]\n",
        "    tok_ = tok[:, :max_len]\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\n",
        "    x1 = Bidirectional(CuDNNLSTM(784, return_sequences=True, name='lstm_layer',))(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(Dropout_new)(x[0])\n",
        "    x2 = Bidirectional(CuDNNLSTM(784, return_sequences=True, name='lstm_layer',))(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
        "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
        "    \n",
        "    # this is required as `model.predict` needs a fixed size!\n",
        "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    \n",
        "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
        "    return model, padded_model    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8SDdKDMEYvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    if (len(a)==0) & (len(b)==0): return 0.5\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqnFcSarEY3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = train.shape[0]\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(train.shape[0]):\n",
        "    \n",
        "    # FIND OVERLAP\n",
        "    all_tweet = train.loc[k,'text']\n",
        "    sel_tweet_bef = train.loc[k,'selected_text']\n",
        "    sel_tweet = preprocess_txt(all_tweet, sel_tweet_bef)\n",
        "    #'  'を探した後に'  'を削除しないとfindがズレる。\n",
        "    #all_tweet = preprocess_all_tw(all_tweet)\n",
        "\n",
        "    text1 = \" \"+\" \".join(all_tweet.split())\n",
        "    text2 = \" \".join(sel_tweet.split())\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1) \n",
        "        \n",
        "    # ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "       # START END TOKENS\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
        "    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
        "    attention_mask[k,:len(enc.ids)+3] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+2] = 1\n",
        "        end_tokens[k,toks[-1]+2] = 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ubx2XMFvAU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7ff0c2d-a4f8-41a0-c04b-4ea8343d4928"
      },
      "source": [
        "%%time\n",
        "jac = []; VER='v0'; DISPLAY=1 \n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start_train = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_end_train = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_split,shuffle=True,random_state=SEED)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model, padded_model = build_model_lstm()\n",
        "        \n",
        "    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
        "    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n",
        "    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
        "    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n",
        "\n",
        "    #cvが小さい方が文字数が少なく、cvが大きい方が文字数が多い\n",
        "    #要はpadが大きい順にソート(文字数が小さい順にソート)\n",
        "    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n",
        "    inpV = [arr[shuffleV] for arr in inpV]\n",
        "    targetV = [arr[shuffleV] for arr in targetV]\n",
        "    \n",
        "    weight_fn = 'drive/My Drive/roberta_lstm2/%s-roberta-%i.h5'%(VER,fold)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        # PAD_IDの数でソート(上のセルを実行してからshuffleを確認)\n",
        "        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n",
        "        # shuffle in batches, kjtherwise short batches will always come in the beginning of each epoch\n",
        "        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n",
        "        batch_inds = np.random.permutation(num_batches)\n",
        "        shuffleT_ = []\n",
        "        for batch_ind in batch_inds:\n",
        "            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n",
        "        shuffleT = np.concatenate(shuffleT_)\n",
        "        inpT = [arr[shuffleT] for arr in inpT]\n",
        "        targetT = [arr[shuffleT] for arr in targetT]\n",
        "        model.fit(inpT, targetT, \n",
        "            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n",
        "            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle \n",
        "        save_weights(model, weight_fn)\n",
        "    \n",
        "    print('Loading model...')\n",
        "    load_weights(model, weight_fn)\n",
        "\n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    print('Predicting all Train for Outlier analysis...')\n",
        "    preds_train = padded_model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n",
        "    preds_start_train += preds_train[0]/skf.n_splits\n",
        "    preds_end_train += preds_train[1]/skf.n_splits\n",
        "    \n",
        "    print('Predicting Test...')\n",
        "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/skf.n_splits\n",
        "    preds_end += preds[1]/skf.n_splits\n",
        "    \n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train.loc[k,'text'] #普通はa<bだが、確率的にそうでない場合もあるから、その場合をハンドリング\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-2:b-1])\n",
        "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "687/687 [==============================] - 143s 209ms/step - loss: 2.8561 - activation_loss: 1.4138 - activation_1_loss: 1.4423 - val_loss: 2.5289 - val_activation_loss: 1.2584 - val_activation_1_loss: 1.2704\n",
            "Epoch 2/2\n",
            "687/687 [==============================] - 113s 165ms/step - loss: 2.5129 - activation_loss: 1.2568 - activation_1_loss: 1.2561 - val_loss: 2.4986 - val_activation_loss: 1.2483 - val_activation_1_loss: 1.2503\n",
            "Epoch 3/3\n",
            "687/687 [==============================] - 114s 166ms/step - loss: 2.3950 - activation_loss: 1.2023 - activation_1_loss: 1.1927 - val_loss: 2.4736 - val_activation_loss: 1.2356 - val_activation_1_loss: 1.2380\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 38s 223ms/step\n",
            "Predicting all Train for Outlier analysis...\n",
            "859/859 [==============================] - 92s 107ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 22s 198ms/step\n",
            ">>>> FOLD 1 Jaccard = 0.7070342347986956\n",
            "\n",
            "#########################\n",
            "### FOLD 2\n",
            "#########################\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - 161s 234ms/step - loss: 2.9249 - activation_loss: 1.4436 - activation_1_loss: 1.4813 - val_loss: 2.5065 - val_activation_loss: 1.2765 - val_activation_1_loss: 1.2300\n",
            "Epoch 2/2\n",
            "688/688 [==============================] - 121s 176ms/step - loss: 2.5168 - activation_loss: 1.2568 - activation_1_loss: 1.2601 - val_loss: 2.4543 - val_activation_loss: 1.2351 - val_activation_1_loss: 1.2192\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - 123s 179ms/step - loss: 2.4169 - activation_loss: 1.2100 - activation_1_loss: 1.2069 - val_loss: 2.4814 - val_activation_loss: 1.2541 - val_activation_1_loss: 1.2273\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 27s 159ms/step\n",
            "Predicting all Train for Outlier analysis...\n",
            "859/859 [==============================] - 78s 91ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 14s 128ms/step\n",
            ">>>> FOLD 2 Jaccard = 0.7098534367766568\n",
            "\n",
            "#########################\n",
            "### FOLD 3\n",
            "#########################\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - 153s 222ms/step - loss: 2.8749 - activation_loss: 1.4186 - activation_1_loss: 1.4563 - val_loss: 2.5158 - val_activation_loss: 1.2718 - val_activation_1_loss: 1.2440\n",
            "Epoch 2/2\n",
            "688/688 [==============================] - 123s 179ms/step - loss: 2.5023 - activation_loss: 1.2586 - activation_1_loss: 1.2436 - val_loss: 2.4967 - val_activation_loss: 1.2408 - val_activation_1_loss: 1.2559\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - 122s 177ms/step - loss: 2.3886 - activation_loss: 1.2037 - activation_1_loss: 1.1849 - val_loss: 2.4990 - val_activation_loss: 1.2404 - val_activation_1_loss: 1.2585\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 28s 162ms/step\n",
            "Predicting all Train for Outlier analysis...\n",
            "859/859 [==============================] - 78s 91ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 14s 128ms/step\n",
            ">>>> FOLD 3 Jaccard = 0.7087365644999007\n",
            "\n",
            "#########################\n",
            "### FOLD 4\n",
            "#########################\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - 158s 229ms/step - loss: 2.9274 - activation_loss: 1.4374 - activation_1_loss: 1.4899 - val_loss: 2.5729 - val_activation_loss: 1.3007 - val_activation_1_loss: 1.2723\n",
            "Epoch 2/2\n",
            "688/688 [==============================] - 129s 187ms/step - loss: 2.5369 - activation_loss: 1.2720 - activation_1_loss: 1.2648 - val_loss: 2.5089 - val_activation_loss: 1.2569 - val_activation_1_loss: 1.2520\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - 126s 183ms/step - loss: 2.4278 - activation_loss: 1.2186 - activation_1_loss: 1.2092 - val_loss: 2.5369 - val_activation_loss: 1.2823 - val_activation_1_loss: 1.2546\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 25s 145ms/step\n",
            "Predicting all Train for Outlier analysis...\n",
            "859/859 [==============================] - 77s 90ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 14s 126ms/step\n",
            ">>>> FOLD 4 Jaccard = 0.6950993396102888\n",
            "\n",
            "#########################\n",
            "### FOLD 5\n",
            "#########################\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - 150s 218ms/step - loss: 2.8849 - activation_loss: 1.4176 - activation_1_loss: 1.4673 - val_loss: 2.5311 - val_activation_loss: 1.2617 - val_activation_1_loss: 1.2693\n",
            "Epoch 2/2\n",
            "688/688 [==============================] - 123s 178ms/step - loss: 2.5326 - activation_loss: 1.2636 - activation_1_loss: 1.2690 - val_loss: 2.5146 - val_activation_loss: 1.2604 - val_activation_1_loss: 1.2542\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - 121s 176ms/step - loss: 2.4099 - activation_loss: 1.2046 - activation_1_loss: 1.2053 - val_loss: 2.4796 - val_activation_loss: 1.2394 - val_activation_1_loss: 1.2402\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 28s 161ms/step\n",
            "Predicting all Train for Outlier analysis...\n",
            "859/859 [==============================] - 77s 90ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 14s 127ms/step\n",
            ">>>> FOLD 5 Jaccard = 0.7038669035882328\n",
            "\n",
            "CPU times: user 30min 28s, sys: 4min 53s, total: 35min 21s\n",
            "Wall time: 48min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcnQ04Em8Erp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fe1cb019-35c6-4bc4-a944-168fdb66cf21"
      },
      "source": [
        "print('>>5Fold CV Jaccard =',np.mean(jac))\n",
        "print(jac) # Jaccard CVs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>5Fold CV Jaccard = 0.704918095854755\n",
            "[0.7070342347986956, 0.7098534367766568, 0.7087365644999007, 0.6950993396102888, 0.7038669035882328]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoxrfWbl8EuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all = []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "    if a>b: \n",
        "        st = test.loc[k,'text']\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        st = tokenizer.decode(enc.ids[a-2:b-1])\n",
        "    all.append(st)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvfFflXl8ExI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_raw = pd.read_csv('test.csv').fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk0zd_5-9NyQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "34831898-3baf-481c-f72c-9534a3040c67"
      },
      "source": [
        "print(test.shape, test_raw.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bcb76b0062c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRUoR_SB9N6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "cec71298-3eee-4427-f333-29e86d2d361d"
      },
      "source": [
        "test['selected_text'] = all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-70f3fbbe15c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWaWrJoS9N-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def rm_url(text): \n",
        "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    return ''.join(text)\n",
        "def rm_underbar(text):\n",
        "    text = re.sub('^_.{2,}?\\s', '', text)\n",
        "    return ''.join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCENuLcm9Vny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_raw = pd.read_csv('/content/test.csv').fillna('')\n",
        "for k in range(test.shape[0]):\n",
        "    if test.loc[k, 'sentiment'] == 'positive' or 'negative':\n",
        "        if len(test.loc[k, 'text'].split()) <= 2:\n",
        "            test.loc[k, 'selected_text'] = test_raw.loc[k, 'text']\n",
        "    #if test.loc[k, 'sentiment'] == 'neutral': \n",
        "        #test.loc[k, 'selected_text'] = test_raw.loc[k, 'text']\n",
        "\n",
        "test['selected_text'] = test['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
        "test['selected_text'] = test['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
        "test['selected_text'] = test['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
        "#test['selected_text'] = test['selected_text'].apply(lambda x: rm_url(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjlZy2q49Vqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "a446478e-81c7-4938-f038-6815a4a5e950"
      },
      "source": [
        "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
        "test.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3444</th>\n",
              "      <td>7d32d4866a</td>\n",
              "      <td>You are sooo lucky. My fiance is away w/the M...</td>\n",
              "      <td>positive</td>\n",
              "      <td>lucky.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>749</th>\n",
              "      <td>200f2b3566</td>\n",
              "      <td>you shouldnt have to reset more than once  if...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>you shouldnt have to reset more than once if ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3136</th>\n",
              "      <td>43c61eb7a3</td>\n",
              "      <td>I can`t find my camera</td>\n",
              "      <td>negative</td>\n",
              "      <td>i can`t find my camera</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>334954f215</td>\n",
              "      <td>hey peoples, dont you just hate being grounded...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>hey peoples, dont you just hate being grounde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3208</th>\n",
              "      <td>2dfbe0b7fb</td>\n",
              "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
              "      <td>negative</td>\n",
              "      <td>down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2095</th>\n",
              "      <td>8986091fd3</td>\n",
              "      <td>wishes happy mothers` day to all moms out there.</td>\n",
              "      <td>positive</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>843</th>\n",
              "      <td>7dd51bdec1</td>\n",
              "      <td>reading Evermore by Lynn Viehl.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>reading evermore by lynn viehl.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>5ff525b74b</td>\n",
              "      <td>Wondering if i cld make things any worse than ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>worse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>f9382a1f7b</td>\n",
              "      <td>oooh, sunshine! A patch of sunshine! And it wi...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>oooh, sunshine! a patch of sunshine! and it w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>930</th>\n",
              "      <td>7720aca199</td>\n",
              "      <td>No, that`s not right - I remember now.  You w...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>no, that`s not right - i remember now. you we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          textID  ...                                      selected_text\n",
              "3444  7d32d4866a  ...                                             lucky.\n",
              "749   200f2b3566  ...   you shouldnt have to reset more than once if ...\n",
              "3136  43c61eb7a3  ...                             i can`t find my camera\n",
              "26    334954f215  ...   hey peoples, dont you just hate being grounde...\n",
              "3208  2dfbe0b7fb  ...                                               down\n",
              "2095  8986091fd3  ...                                              happy\n",
              "843   7dd51bdec1  ...                    reading evermore by lynn viehl.\n",
              "172   5ff525b74b  ...                                              worse\n",
              "100   f9382a1f7b  ...   oooh, sunshine! a patch of sunshine! and it w...\n",
              "930   7720aca199  ...   no, that`s not right - i remember now. you we...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KWuwVfm9Vs6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef173c16-b452-45f6-83b0-31a565ceaadf"
      },
      "source": [
        "pytorch_score = '''\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.1321 | Jaccard: 0.6634\n",
        "Epoch 1/3 |  val  | Loss: 1.6480 | Jaccard: 0.7166\n",
        "Epoch 2/3 | train | Loss: 1.6013 | Jaccard: 0.7182\n",
        "Epoch 2/3 |  val  | Loss: 1.6582 | Jaccard: 0.7059\n",
        "Epoch 3/3 | train | Loss: 1.4530 | Jaccard: 0.7373\n",
        "Epoch 3/3 |  val  | Loss: 1.7208 | Jaccard: 0.7131\n",
        "Fold: 2\n",
        "Epoch 1/3 | train | Loss: 2.1968 | Jaccard: 0.6553\n",
        "Epoch 1/3 |  val  | Loss: 1.7515 | Jaccard: 0.6925\n",
        "Epoch 2/3 | train | Loss: 1.6998 | Jaccard: 0.7068\n",
        "Epoch 2/3 |  val  | Loss: 1.6465 | Jaccard: 0.7124\n",
        "Epoch 3/3 | train | Loss: 1.5439 | Jaccard: 0.7252\n",
        "Epoch 3/3 |  val  | Loss: 1.6395 | Jaccard: 0.7160\n",
        "Fold: 3\n",
        "Epoch 1/3 | train | Loss: 2.1237 | Jaccard: 0.6675\n",
        "Epoch 1/3 |  val  | Loss: 1.7006 | Jaccard: 0.7078\n",
        "Epoch 2/3 | train | Loss: 1.6322 | Jaccard: 0.7172\n",
        "Epoch 2/3 |  val  | Loss: 1.6523 | Jaccard: 0.7141\n",
        "Epoch 3/3 | train | Loss: 1.4792 | Jaccard: 0.7337\n",
        "Epoch 3/3 |  val  | Loss: 1.6673 | Jaccard: 0.7169\n",
        "Fold: 4\n",
        "Epoch 1/3 | train | Loss: 2.1617 | Jaccard: 0.6601\n",
        "Epoch 1/3 |  val  | Loss: 1.5938 | Jaccard: 0.7212\n",
        "Epoch 2/3 | train | Loss: 1.6270 | Jaccard: 0.7123\n",
        "Epoch 2/3 |  val  | Loss: 1.5249 | Jaccard: 0.7308\n",
        "Epoch 3/3 | train | Loss: 1.4600 | Jaccard: 0.7373\n",
        "Epoch 3/3 |  val  | Loss: 1.5597 | Jaccard: 0.7247\n",
        "Fold: 5\n",
        "Epoch 1/3 | train | Loss: 2.0595 | Jaccard: 0.6713\n",
        "Epoch 1/3 |  val  | Loss: 1.6322 | Jaccard: 0.7195\n",
        "Epoch 2/3 | train | Loss: 1.5929 | Jaccard: 0.7210\n",
        "Epoch 2/3 |  val  | Loss: 1.6388 | Jaccard: 0.7158\n",
        "Epoch 3/3 | train | Loss: 1.5069 | Jaccard: 0.7328\n",
        "Epoch 3/3 |  val  | Loss: 1.6546 | Jaccard: 0.7208\n",
        "Fold: 6\n",
        "Epoch 1/3 | train | Loss: 2.0796 | Jaccard: 0.6708\n",
        "Epoch 1/3 |  val  | Loss: 1.6544 | Jaccard: 0.7078\n",
        "Epoch 2/3 | train | Loss: 1.5919 | Jaccard: 0.7180\n",
        "Epoch 2/3 |  val  | Loss: 1.6450 | Jaccard: 0.7173\n",
        "Epoch 3/3 | train | Loss: 1.4351 | Jaccard: 0.7392\n",
        "Epoch 3/3 |  val  | Loss: 1.6319 | Jaccard: 0.7206\n",
        "Fold: 7\n",
        "Epoch 1/3 | train | Loss: 2.1183 | Jaccard: 0.6655\n",
        "Epoch 1/3 |  val  | Loss: 1.5971 | Jaccard: 0.7187\n",
        "Epoch 2/3 | train | Loss: 1.6268 | Jaccard: 0.7120\n",
        "Epoch 2/3 |  val  | Loss: 1.5684 | Jaccard: 0.7219\n",
        "Epoch 3/3 | train | Loss: 1.4952 | Jaccard: 0.7317\n",
        "Epoch 3/3 |  val  | Loss: 1.5916 | Jaccard: 0.7231\n",
        "Fold: 8\n",
        "Epoch 1/3 | train | Loss: 2.1916 | Jaccard: 0.6603\n",
        "Epoch 1/3 |  val  | Loss: 1.6748 | Jaccard: 0.7050\n",
        "Epoch 2/3 | train | Loss: 1.6450 | Jaccard: 0.7120\n",
        "Epoch 2/3 |  val  | Loss: 1.6062 | Jaccard: 0.7196\n",
        "Epoch 3/3 | train | Loss: 1.5097 | Jaccard: 0.7304\n",
        "Epoch 3/3 |  val  | Loss: 1.6520 | Jaccard: 0.7165\n",
        "Fold: 9\n",
        "Epoch 1/3 | train | Loss: 2.1056 | Jaccard: 0.6662\n",
        "Epoch 1/3 |  val  | Loss: 1.7136 | Jaccard: 0.7122\n",
        "Epoch 2/3 | train | Loss: 1.6063 | Jaccard: 0.7147\n",
        "Epoch 2/3 |  val  | Loss: 1.6593 | Jaccard: 0.7206\n",
        "Epoch 3/3 | train | Loss: 1.4619 | Jaccard: 0.7339\n",
        "Epoch 3/3 |  val  | Loss: 1.6499 | Jaccard: 0.7212\n",
        "Fold: 10\n",
        "Epoch 1/3 | train | Loss: 2.0821 | Jaccard: 0.6699\n",
        "Epoch 1/3 |  val  | Loss: 1.6264 | Jaccard: 0.7104\n",
        "Epoch 2/3 | train | Loss: 1.5766 | Jaccard: 0.7236\n",
        "Epoch 2/3 |  val  | Loss: 1.6017 | Jaccard: 0.7184\n",
        "Epoch 3/3 | train | Loss: 1.4345 | Jaccard: 0.7424\n",
        "Epoch 3/3 |  val  | Loss: 1.6217 | Jaccard: 0.7186\n",
        "\n",
        "'''\n",
        "import re \n",
        "import numpy as np\n",
        "\n",
        "scores = re.findall(r'0\\.\\d{4}', text)\n",
        "scores = [float(i) for i in scores]\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CA6Rl6i8Jdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}